# derivative of likelihood function: u(beta.j)
log.likelihd.der <- function(beta) {
delta <- x.hat.beta + X_r * rep(beta, rep(n,hat.G0.t.len))
pi_r <- 1/(1+exp(-delta)) # 1-\pi_r
f <- colSums(X_r[gamma_y1,]*(1-pi_r[gamma_y1,])) - colSums(X_r[gamma_y0,]*pi_r[gamma_y0,]) - beta/lambda
# f1 <- colSums((y-pi_r)*X_r) - beta/lambda
return(f)
}
# log of approximate marginal posterior likelihood function for nbd+:log(\tilde S(r))
log.tilde.S.plus <- function(beta) {
delta <- x.hat.beta + X_r * rep(beta, rep(n,hat.G0.t.len))
# cdf.delta.log <- pnorm(delta,0,1,log.p = T, lower.tail = TRUE)
f = colSums(delta[gamma_y1,])- colSums(log(1+exp(delta))) -
0.5/lambda*(rep(crossprod(hat.beta.t),p-k) + beta^2) - (k+1)/2*log(n*lambda) - lchoose(p, k+1)
return(f)
}
# log of exact marginal posterior likelihood function: log(S(r))
# x0 comes first
neg.log.exact.likelihd <- function(beta) {
X.r = x0
len.beta <- length(beta)
delta <- X.r%*%as.matrix(beta) # n times 1 vector
f = sum(y*delta - log(1+exp(delta)))- len.beta/2*log(2*pi*lambda) -
crossprod(beta)/(2*lambda) -  lchoose(p, len.beta)
return(-f)
}
set.seed(1000)
var.true <- c(1,3,5)
k.true=length(var.true)
Beta = rep(0,p)
beta.true <- c(0.8,1,-1)
Beta[var.true] = beta.true
lambda = 10000
rho = 0.6
# (n,p) = 100,500; 300,1000; 500,1500
n=300
p=1000
full.model <- 1:p
Sig_x = rho^(abs(matrix(full.model,p,p)-t(matrix(full.model,p,p))))
set.seed(1000)
var.true <- c(1,3,5)
k.true=length(var.true)
Beta = rep(0,p)
beta.true <- c(0.8,1,-1)
Beta[var.true] = beta.true
var.false <- setdiff((full.model),var.true)
gamma.true <- rep(0,p)
gamma.true[var.true] <- 1
X = rmvnorm(n,rep(0,p),Sig_x,method="chol")
X[,1]<-rep(1,n)
library(rootSolve)
library(mvtnorm)
library(xtable)
X = rmvnorm(n,rep(0,p),Sig_x,method="chol")
X[,1]<-rep(1,n)
eta = X%*%Beta
pi.true <- 1/(1+exp(-eta))
y = rbinom(n,size = 1,prob = pi.true)
gamma_y1 <- which(y==1)
gamma_y0 <- (1:n)[-gamma_y1]
#### GLM-Poission regression ####
x0 <- X[, var.true]
hat.beta.true.optim <- optim(par = rep(0,length(var.true)), fn = neg.log.exact.likelihd, method = 'BFGS')
hat.beta.true <- hat.beta.true.optim$par
post.margin.true <- -hat.beta.true.optim$value
summary(glm(y~X[, var.true]-1,family="binomial"))
t1 <- Sys.time()
iter.num <- 1
num_break = 0
# initial model with |r| = 1
hat.G1 <- 1
hat.G1.t <- hat.G1
hat.G0.t <- full.model[-hat.G1.t]
X.r <- X[, hat.G1.t]
x0 <- X.r
likelihd.optim <- optim(par = rep(0,length(hat.G1.t)), fn = neg.log.exact.likelihd, method = 'BFGS')
log.S <- -likelihd.optim$value
hat.beta.t <- likelihd.optim$par
hat.beta.t
likelihd.optim
bookdown:::serve_book()
bookdown:::mathquill()
bookdown:::serve_book()
bookdown:::serve_book()
bookdown:::serve_book()
library(rootSolve)
library(mvtnorm)
library(xtable)
# derivative of likelihood function: u(beta.j)
log.likelihd.der <- function(beta) {
delta <- x.hat.beta + X_r * rep(beta, rep(n,hat.G0.t.len))
pi_r <- 1/(1+exp(-delta)) # 1-\pi_r
f <- colSums(X_r[gamma_y1,]*(1-pi_r[gamma_y1,])) - colSums(X_r[gamma_y0,]*pi_r[gamma_y0,]) - beta/lambda
# f1 <- colSums((y-pi_r)*X_r) - beta/lambda
return(f)
}
# log of approximate marginal posterior likelihood function for nbd+:log(\tilde S(r))
log.tilde.S.plus <- function(beta) {
delta <- x.hat.beta + X_r * rep(beta, rep(n,hat.G0.t.len))
# cdf.delta.log <- pnorm(delta,0,1,log.p = T, lower.tail = TRUE)
f = colSums(delta[gamma_y1,])- colSums(log(1+exp(delta))) -
0.5/lambda*(rep(crossprod(hat.beta.t),p-k) + beta^2) - (k+1)/2*log(n*lambda) - lchoose(p, k+1)
return(f)
}
# log of exact marginal posterior likelihood function: log(S(r))
# x0 comes first
neg.log.exact.likelihd <- function(beta) {
X.r = x0
len.beta <- length(beta)
delta <- X.r%*%as.matrix(beta) # n times 1 vector
f = sum(y*delta - log(1+exp(delta)))- len.beta/2*log(2*pi*lambda) -
crossprod(beta)/(2*lambda) -  lchoose(p, len.beta)
return(-f)
}
lambda = 10000
rho = 0.6
# (n,p) = 100,500; 300,1000; 500,1500
n=300
p=1000
full.model <- 1:p
Sig_x = rho^(abs(matrix(full.model,p,p)-t(matrix(full.model,p,p))))
set.seed(1000)
var.true <- c(1,3,5)
k.true=length(var.true)
Beta = rep(0,p)
beta.true <- c(0.8,1,-1)
Beta[var.true] = beta.true
var.false <- setdiff((full.model),var.true)
gamma.true <- rep(0,p)
gamma.true[var.true] <- 1
X = rmvnorm(n,rep(0,p),Sig_x,method="chol")
X[,1]<-rep(1,n)
eta = X%*%Beta
pi.true <- 1/(1+exp(-eta))
y = rbinom(n,size = 1,prob = pi.true)
gamma_y1 <- which(y==1)
gamma_y0 <- (1:n)[-gamma_y1]
#### GLM-Poission regression ####
x0 <- X[, var.true]
hat.beta.true.optim <- optim(par = rep(0,length(var.true)), fn = neg.log.exact.likelihd, method = 'BFGS')
hat.beta.true <- hat.beta.true.optim$par
post.margin.true <- -hat.beta.true.optim$value
# summary(glm(y~X[, var.true]-1,family="binomial"))
t1 <- Sys.time()
iter.num <- 1
num_break = 0
# initial model with |r| = 1
hat.G1 <- 1
hat.G1.t <- hat.G1
hat.G0.t <- full.model[-hat.G1.t]
X.r <- X[, hat.G1.t]
x0 <- X.r
likelihd.optim <- optim(par = rep(0,length(hat.G1.t)), fn = neg.log.exact.likelihd, method = 'BFGS')
log.S <- -likelihd.optim$value
hat.beta.t <- likelihd.optim$par
hat.beta.t
likelihd.optim
## Model setup
library(rootSolve)
library(mvtnorm)
n=300
p=1000
lambda = 10000
full.model <- 1:p
rho = 0.6
Sig_x = rho^(abs(matrix(full.model,p,p)-t(matrix(full.model,p,p))))
set.seed(1000)
var.true <- c(1,3,5)
k.true=length(var.true)
Beta = rep(0,p)
beta.true <- c(0.8,1,-1)
Beta[var.true] = beta.true
var.false <- setdiff((full.model),var.true)
X = rmvnorm(n,rep(0,p),Sig_x,method="chol")
X[,1]<-rep(1,n)
eta = X%*%Beta
pi.true <- 1/(1+exp(-eta))
y = rbinom(n,size = 1,prob = pi.true)
y
plot(y)
plot(y)
model1 <- glm(y ~ X[,var.true], family = "binomial")
model1
X[,var.true]
model1 <- glm(y ~ X[,var.true]-1, family = "binomial")
model1
rho = 0.2
Sig_x = rho^(abs(matrix(full.model,p,p)-t(matrix(full.model,p,p))))
y = rbinom(n,size = 1,prob = pi.true)
model1 <- glm(y ~ X[,var.true]-1, family = "binomial")
X = rmvnorm(n,rep(0,p),Sig_x,method="chol")
X[,1]<-rep(1,n)
eta = X%*%Beta
pi.true <- 1/(1+exp(-eta))
y = rbinom(n,size = 1,prob = pi.true)
model1 <- glm(y ~ X[,var.true]-1, family = "binomial")
## Model setup
library(rootSolve)
library(mvtnorm)
n=300
p=1000
lambda = 10000
full.model <- 1:p
rho = 0.2
Sig_x = rho^(abs(matrix(full.model,p,p)-t(matrix(full.model,p,p))))
set.seed(1000)
var.true <- c(1,3,5)
k.true=length(var.true)
Beta = rep(0,p)
beta.true <- c(0.8,1,-1)
Beta[var.true] = beta.true
X = rmvnorm(n,rep(0,p),Sig_x,method="chol")
X[,1]<-rep(1,n)
eta = X%*%Beta
pi.true <- 1/(1+exp(-eta))
y = rbinom(n,size = 1,prob = pi.true)
model1 <- glm(y ~ X[,var.true]-1, family = "binomial")
model1
## Model setup
library(rootSolve)
library(mvtnorm)
n=300
p=1000
lambda = 10000
full.model <- 1:p
rho = 0.2
Sig_x = rho^(abs(matrix(full.model,p,p)-t(matrix(full.model,p,p))))
set.seed(1000)
var.true <- c(1,5,10)
k.true=length(var.true)
Beta = rep(0,p)
beta.true <- c(0.8,1,1.5)
Beta[var.true] = beta.true
X = rmvnorm(n,rep(0,p),Sig_x,method="chol")
X[,1]<-rep(1,n)
eta = X%*%Beta
pi.true <- 1/(1+exp(-eta))
y = rbinom(n,size = 1,prob = pi.true)
model1 <- glm(y ~ X[,var.true]-1, family = "binomial")
model1
## Model setup
library(rootSolve)
library(mvtnorm)
n=300
p=1000
lambda = 10000
full.model <- 1:p
rho = 0.2
Sig_x = rho^(abs(matrix(full.model,p,p)-t(matrix(full.model,p,p))))
set.seed(1000)
var.true <- c(1,5,10)
k.true=length(var.true)
Beta = rep(0,p)
beta.true <- c(1,1.5,2)
Beta[var.true] = beta.true
X = rmvnorm(n,rep(0,p),Sig_x,method="chol")
X[,1]<-rep(1,n)
eta = X%*%Beta
pi.true <- 1/(1+exp(-eta))
y = rbinom(n,size = 1,prob = pi.true)
model1 <- glm(y ~ X[,var.true]-1, family = "binomial")
model1
# Functions
gamma_y1 <- which(y==1)
gamma_y0 <- (1:n)[-gamma_y1]
# the first derivative of log-likelihood function l(theta_j): u_j(beta_j) in (1.2)
log.likelihd.der <- function(beta) {
delta <- x.hat.beta + X_r * rep(beta, rep(n,hat.G0.t.len))
pi_r <- 1/(1+exp(-delta)) # 1-\pi_r
f <- colSums(X_r[gamma_y1,]*(1-pi_r[gamma_y1,])) - colSums(X_r[gamma_y0,]*pi_r[gamma_y0,]) - beta/lambda
return(f)
}
# log of approximate marginal posterior likelihood function for nbd+:log(\tilde S(r))
log.tilde.S.plus <- function(beta) {
delta <- x.hat.beta + X_r * rep(beta, rep(n,hat.G0.t.len))
# cdf.delta.log <- pnorm(delta,0,1,log.p = T, lower.tail = TRUE)
f = colSums(delta[gamma_y1,])- colSums(log(1+exp(delta))) -
0.5/lambda*(rep(crossprod(hat.beta.t),p-k) + beta^2) - (k+1)/2*log(n*lambda) - lchoose(p, k+1)
return(f)
}
# log of exact marginal posterior likelihood function: log(S(r))
# x0 comes first
neg.log.exact.likelihd <- function(beta) {
X.r = x0
len.beta <- length(beta)
delta <- X.r%*%as.matrix(beta) # n times 1 vector
f = sum(y*delta - log(1+exp(delta)))- len.beta/2*log(2*pi*lambda) -
crossprod(beta)/(2*lambda) -  lchoose(p, len.beta)
return(-f)
}
# initial model is the intercept
hat.G1 <- 1
hat.G1.t <- hat.G1
hat.G0.t <- full.model[-hat.G1.t]
X.r <- X[, hat.G1.t]
x0 <- X.r
likelihd.optim <- optim(par = rep(0,length(hat.G1.t)), fn = neg.log.exact.likelihd, method = 'BFGS')
log.S <- -likelihd.optim$value
hat.beta.t <- likelihd.optim$par
hat.G0.t <- full.model[-hat.G1.t]
k = length(hat.G1.t) # num of predictor
X.r <- X[, hat.G1.t]
X_r <- X[, hat.G0.t]
## 2.ii)
add.nei <- lapply(hat.G0.t, FUN = function(x) sort(c(hat.G1.t, x)))
### N_-: deletion neighbor
if (k>1) {
del.nei <- lapply(2:k, FUN = function(x) hat.G1.t[-x]) # not delete intercept
} else {
del.nei <- NULL
}
## 2.iii) & iv)
### \tilde s(r) in N_+: addition neighbor
hat.G0.t.len <- length(hat.G0.t)
x.hat.beta <- matrix(X.r%*%as.matrix(hat.beta.t),n,hat.G0.t.len)
tilde.beta.plus <- multiroot(f = log.likelihd.der, start = rep(0,hat.G0.t.len),
jactype = 'bandint', bandup = 0, banddown = 0)$root
appro.post.plus <- log.tilde.S.plus(tilde.beta.plus)
appro.post.plus
plot(appro.post.plus)
bookdown:::serve_book()
# initial model is the first predictor
hat.G1 <- 1 # first predictor
hat.G1.t <- hat.G1
hat.G0.t <- full.model[-hat.G1.t]
X.r <- X[, hat.G1.t]
x0 <- X.r
likelihd.optim <- optim(par = rep(0,length(hat.G1.t)), fn = neg.log.exact.likelihd, method = 'BFGS')
log.S <- -likelihd.optim$value
hat.beta.t <- likelihd.optim$par
hat.G0.t <- full.model[-hat.G1.t]
k = length(hat.G1.t) # num of predictor
X.r <- X[, hat.G1.t]
X_r <- X[, hat.G0.t]
## 2.ii)
# Define nbd+(r): addition neighbor
add.nei <- lapply(hat.G0.t, FUN = function(x) sort(c(hat.G1.t, x)))
### Define nbd-(r): deletion neighbor
if (k>1) {
del.nei <- lapply(2:k, FUN = function(x) hat.G1.t[-x]) # reserve first predictor always
} else {
del.nei <- NULL
}
## 2.iii) & iv)
### \tilde s(r) in nbd_+(r): addition neighbor
hat.G0.t.len <- length(hat.G0.t)
x.hat.beta <- matrix(X.r%*%as.matrix(hat.beta.t),n,hat.G0.t.len)
tilde.beta.plus <- multiroot(f = log.likelihd.der, start = rep(0,hat.G0.t.len),
jactype = 'bandint', bandup = 0, banddown = 0)$root
appro.post.plus <- log.tilde.S.plus(tilde.beta.plus) # Eq.(1.6)
### \tilde s(r) in N_-: deletion neighbor
if (k>1) {
appro.post.minus <- rep(NA,k-1)
for (j in 2:k) {# begins from 2 to avoid removal of the intercept
x0 = X[,hat.G1.t[-j]]
appro.post.minus[j-1] <- -neg.log.exact.likelihd(hat.beta.t[-j])
}
} else appro.post.minus = NULL
appro.post.minus
dim(x.hat.theta)
## Model setup
library(rootSolve) # for multiroot function
library(mvtnorm)
n=300 # sample size
p=1000 # dimension
lambda = 10000 # hyper-parameter in the prior
full.model <- 1:p
rho = 0.2 # correlation between each pair of adjacent covariates
# covariance matrix for covariate matrix: X.
Sig_x = rho^(abs(matrix(full.model,p,p)-t(matrix(full.model,p,p))))
set.seed(1000) # set random seed
var.true <- c(1,5,10) # true model r^*
k.true=length(var.true) # size of true model r^*
Theta = rep(0,p)
theta.true <- c(1,1.5,2) # values of true theta_r values
Theta[var.true] = theta.true
X = rmvnorm(n,rep(0,p),Sig_x,method="chol") # generate X from multivar-normal distribution
X[,1]<-rep(1,n) # make first column of X a 1 vector
eta = X%*%Theta
pi.true <- 1/(1+exp(-eta)) # prob of y = 1.
y = rbinom(n,size = 1,prob = pi.true) # response is from Bernoulli distribution.
# Functions
gamma_y1 <- which(y==1) # position of y=1
gamma_y0 <- (1:n)[-gamma_y1] # position of y=0
# log.likelihd.der is the first derivative of log-likelihood function of
# l(theta_j): u_j(theta_j) in Eq.(1.3). The input "theta" is vectorized.
# psi is a vector consisting of psi_i(theta_j)'s for all j's.
log.likelihd.der <- function(theta) {
psi <- x.hat.theta + X_r * rep(theta, rep(n,hat.G0.t.len))
pi_r <- 1/(1+exp(-psi))
f <- colSums(X_r[gamma_y1,]*(1-pi_r[gamma_y1,])) -
colSums(X_r[gamma_y0,]*pi_r[gamma_y0,]) - theta/lambda
return(f)
}
# log of approximate marginal posterior likelihood function
# for nbd+:log(\tilde S(r)) in Eq.(1.6)
log.tilde.S.plus <- function(theta) {
psi <- x.hat.theta + X_r * rep(theta, rep(n,hat.G0.t.len))
f = colSums(psi[gamma_y1,])- colSums(log(1+exp(psi))) -
0.5/lambda*(rep(crossprod(hat.theta.t),p-k) + theta^2) - (k+1)/2*log(n*lambda)
- lchoose(p, k+1)
return(f)
}
# log of exact marginal posterior likelihood function: -Eq.(1.1)
neg.log.exact.likelihd <- function(theta) {
X.r = x0
len.theta <- length(theta)
XTheta.r <- X.r%*%as.matrix(theta) # n times 1 vector
f = sum(y*XTheta.r - log(1+exp(XTheta.r)))- len.theta/2*log(2*pi*lambda) -
crossprod(theta)/(2*lambda) -  lchoose(p, len.theta)
return(-f)
}
# Initialize the current model is the first predictor
hat.G1 <- 1 # first predictor
hat.G1.t <- hat.G1
hat.G0.t <- full.model[-hat.G1.t] # comlement set of the current model.
X.r <- X[, hat.G1.t] # predictors in the current model
X_r <- X[, hat.G0.t] # predictors not in the current model.
k = length(hat.G1.t) # num of predictors in current model
x0 <- X.r
# estimate posterior model \hat\theta_r for current model
likelihd.optim <- optim(par = rep(0,length(hat.G1.t)), fn = neg.log.exact.likelihd,
method = 'BFGS')
log.S <- -likelihd.optim$value
hat.theta.t <- likelihd.optim$par
## 2.ii)
# Define nbd+(r): addition neighbor
add.nei <- lapply(hat.G0.t, FUN = function(x) sort(c(hat.G1.t, x)))
### Define nbd-(r): deletion neighbor
if (k>1) {
del.nei <- lapply(2:k, FUN = function(x) hat.G1.t[-x]) # reserve first predictor always
} else {
del.nei <- NULL # if k=1, there is no deletion neighbor.
}
## 2.iii) & iv)
### \tilde s(r) in nbd_+(r): addition neighbor
hat.G0.t.len <- length(hat.G0.t) # num of elements in complement set of current model
x.hat.theta <- matrix(X.r%*%as.matrix(hat.theta.t),n,hat.G0.t.len)
# solutions to system of equations.
tilde.theta.plus <- multiroot(f = log.likelihd.der, start = rep(0,hat.G0.t.len),
jactype = 'bandint', bandup = 0, banddown = 0)$root
appro.post.plus <- log.tilde.S.plus(tilde.theta.plus) # Eq.(1.6)
### \tilde s(r) in nbd-(r): deletion neighbor of current model.
if (k>1) {
appro.post.minus <- rep(NA,k-1)
for (j in 2:k) {# begins from 2 to avoid removal of the intercept
x0 = X[,hat.G1.t[-j]]
appro.post.minus[j-1] <- -neg.log.exact.likelihd(hat.theta.t[-j])
}
} else appro.post.minus = NULL # if k=1, no deletion neighbor.
appro.post.plus
plot(appro.post.plus)
which.max(appro.post.plus)
which.max(appro.post.plus)
plot(appro.post.plus)
add.nei[which.max(appro.post.plus)]
hat.r.plus <- add.nei[which.max(appro.post.plus)] #
hat.r.plus
hat.r.plus <- add.nei[[which.max(appro.post.plus)]] #
add.nei[[which.max(appro.post.plus)]]
hat.r.plus <- add.nei[[which.max(appro.post.plus)]]
library(microbenchmark)
LOG.exact.likelihd <- rep(NA,length(neighbor))
neighbor <- c(add.nei, del.nei)
LOG.exact.likelihd <- rep(NA,length(add.nei))
j = 1
timecost <- microbenchmark("for_loop" = {
for (model in neighbor) {
x0 <- X[,model]
likelihd.optim <- optim(par = rep(0,length(model)), fn = neg.log.exact.likelihd, method = 'BFGS')
LOG.exact.likelihd[j] <- - likelihd.optim$value
j = j + 1
}
},
"Proposed" = {
tilde.theta.plus <- multiroot(f = log.likelihd.der, start = rep(0,hat.G0.t.len),
jactype = 'bandint', bandup = 0, banddown = 0)$root
}
)
timecost
timecost
953/1000*100
multiroot(f = log.likelihd.der, start = rep(0,hat.G0.t.len),
jactype = 'bandint', bandup = 0, banddown = 0)$root
neighbor
add.nei
add.nei
